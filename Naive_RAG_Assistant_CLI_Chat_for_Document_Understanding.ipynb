{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasreman8/LLMs-For-RAGs/blob/main/Naive_RAG_Assistant_CLI_Chat_for_Document_Understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Objective\n",
        "\n",
        "- Build an LLM assistant for document-based Q&A using retrieval-augmented generation."
      ],
      "metadata": {
        "id": "N-SSaZ34Ejlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "UgAFFFruPmpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - LangChain is an open-source framework designed to facilitate the development of applications that leverage LLMs such as OpenAI's GPT-3 and GPT-4."
      ],
      "metadata": {
        "id": "uwyUhZU_dlOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install -q openai==1.66.3 \\\n",
        "                langchain==0.3.20 \\\n",
        "                langchain-community==0.3.19 \\\n",
        "                langchain-chroma==0.2.2 \\\n",
        "                langchain-openai==0.3.9 \\\n",
        "                chromadb==0.6.3 \\\n",
        "                posthog==2.4.2"
      ],
      "metadata": {
        "id": "4mrHRg4eV_hr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zuVrrCykDkmM"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "from google.colab import userdata\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get('my_api_key')\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        ")\n",
        "\n",
        "model_name = 'gpt-4o-mini'\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    api_key=openai_api_key,\n",
        "    model='text-embedding-3-small'\n",
        ")"
      ],
      "metadata": {
        "id": "v3TzU6ZpnxD-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Vector Database"
      ],
      "metadata": {
        "id": "Lqic3jajEMZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we persisted the database to to a folder, we can upload this database to this Colab instance and point a Chroma instance to this database."
      ],
      "metadata": {
        "id": "7B_do80GZXDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, the database is maintained as a separate entity and CRUD operations are managed just as one would for normal databases (e.g., relational databases)."
      ],
      "metadata": {
        "id": "lCGSIZifZ8jF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the database is uploaded onto the Colab instance, we can unzip it and attach a retriever."
      ],
      "metadata": {
        "id": "8Y_-X6UYZxr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tesla_db.zip"
      ],
      "metadata": {
        "id": "10V87q5WuMc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7ccdf8-5a7c-4a37-df95-bc88ef4ac309"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  tesla_db.zip\n",
            "replace tesla_db/chroma.sqlite3? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: tesla_db/chroma.sqlite3  \n",
            "   creating: tesla_db/e5401e39-e25e-4be0-8e53-57cb38e8c579/\n",
            "  inflating: tesla_db/e5401e39-e25e-4be0-8e53-57cb38e8c579/header.bin  \n",
            "  inflating: tesla_db/e5401e39-e25e-4be0-8e53-57cb38e8c579/link_lists.bin  \n",
            "  inflating: tesla_db/e5401e39-e25e-4be0-8e53-57cb38e8c579/length.bin  \n",
            "  inflating: tesla_db/e5401e39-e25e-4be0-8e53-57cb38e8c579/index_metadata.pickle  \n",
            "  inflating: tesla_db/e5401e39-e25e-4be0-8e53-57cb38e8c579/data_level0.bin  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting the client to our database\n",
        "chromadb_client = chromadb.PersistentClient(\n",
        "    path=\"./tesla_db\"\n",
        ")\n",
        "\n",
        "# this is the step where a LangChain vector is “mounted” on top of on-disk Chroma DB and point it at a specific collection for the Tesla 10-Ks.\n",
        "tesla_10k_collection = 'tesla-10k-2019-to-2023'\n",
        "\n",
        "vectorstore_persisted = Chroma(\n",
        "    collection_name=tesla_10k_collection,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    embedding_function=embedding_model,\n",
        "    client=chromadb_client,\n",
        "    persist_directory=\"./tesla_db\"\n",
        ")"
      ],
      "metadata": {
        "id": "_LuDOQpxumYO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turning the Chroma vector store into a LangChain retriever and telling it to do a plain k-NN similarity search that returns the top 5 most similar chunks.\n",
        "retriever = vectorstore_persisted.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': 5}\n",
        ")"
      ],
      "metadata": {
        "id": "C-15bwukuVYU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Q&A"
      ],
      "metadata": {
        "id": "pv9oH4ukXLSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Design"
      ],
      "metadata": {
        "id": "s343-PgW6P-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RAG system message should clearly communicate to the LLM that the input will include a user query along with the necessary context information to address that query. Additionally, the response should rely solely on the context information provided."
      ],
      "metadata": {
        "id": "DFSzXhkA4Mmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qna_system_message = \"\"\"\n",
        "You are an assistant to a financial services firm who answers user queries on annual reports.\n",
        "User input will have the context required by you to answer user queries.\n",
        "This context will be delimited by: <Context> and </Context>.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User queries will be delimited by: <Question> and </Question>.\n",
        "\n",
        "Please answer user queries only using the context provided in the input.\n",
        "Do not mention anything about the context in your final answer. Your response should only contain the answer to the question.\n",
        "\n",
        "If the answer is not found in the context, respond \"I don't know\".\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LR4dzgL96U0-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_user_message_template = \"\"\"\n",
        "<Context>\n",
        "Here are some documents that are relevant to the question mentioned below.\n",
        "{context}\n",
        "</Context>\n",
        "\n",
        "<Question>\n",
        "{question}\n",
        "</Question>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bDexqi8c6Xmm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving relevant documents"
      ],
      "metadata": {
        "id": "iT4QahJv6xTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What was the annual revenue of the company in 2022?\""
      ],
      "metadata": {
        "id": "nsZuE-Xo2dAR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code snippet runs a semantic search and returns the top-k chunks (as Document objects) from Tesla-10k-2019-to-2023 collection\n",
        "relevant_document_chunks = retriever.invoke(user_query)\n",
        "\n",
        "len(relevant_document_chunks)"
      ],
      "metadata": {
        "id": "MUBRJsi12e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fef781-5935-419e-b450-97805a253de3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect the first document like so:"
      ],
      "metadata": {
        "id": "fcKi88RowP8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for document in relevant_document_chunks:\n",
        "    print(document.page_content.replace(\"\\t\", \" \"))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KeoZOE62jF5",
        "outputId": "c17a026c-eea3-405c-f214-705073164c71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "systems.\n",
            "In 2020, we recognized total revenues of $31.54 billion, representing an increase of $6.96 billion compared to the prior year. We continue to ramp\n",
            "production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue\n",
            "growth.\n",
            "In 2020, our net income attributable to common stockholders was $721 million, representing a favorable change of $1.58 billion compared to the prior\n",
            "year. In 2020, our operating margin was 6.3%, representing a favorable change of 6.6% compared to the prior year. We continue to focus on operational\n",
            "efficiencies, while we have seen an acceleration of non-cash stock-based compensation expense due to a rapid increase in our market capitalization and updates\n",
            "to our business outlook.\n",
            "We ended 2020 with $19.38 billion in cash and cash equivalents, representing an increase of $13.12 billion from the end of 2019. Our cash flows from\n",
            "operating activities during 2020 was $5.94 billion, compared to $2.41 billion during 2019, and capital expenditures amounted to $3.16 billion during 2020,\n",
            "compared to $1.33 billion during 2019. Sustained growth has allowed our business to generally fund itself, but we will continue a number of capital-intensive\n",
            "projects in upcoming periods.\n",
            "Management Opportunities, Challenges and Risks and 2021 Outlook\n",
            "Impact of COVID-19 Pandemic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Composing the response"
      ],
      "metadata": {
        "id": "G_ekBjVM60P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compose the response to user queries, we assemble the prompt that uses the system message defined above and the dynamically retrieved context for the user query."
      ],
      "metadata": {
        "id": "mO6P2A3J4pkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What was the annual revenue of the company in 2022?\""
      ],
      "metadata": {
        "id": "4WZhSBsI7Av7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " This is a classic RAG (Retrieval Augmented Generation) call:\n",
        "  - It retrieves relevant 10K chunks,\n",
        "  - builds a context block,\n",
        "  - stuffs that into a chat prompt,\n",
        "  - calls the model with temperature = 0 (deterministic -> creativity 0),\n",
        "  - prints the model's answer."
      ],
      "metadata": {
        "id": "hFawlpGjlHR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_document_chunks = retriever.invoke(user_query)\n",
        "context_list = [d.page_content for d in relevant_document_chunks]\n",
        "context_for_query = \"\\n---\\n\".join(context_list)\n",
        "\n",
        "prompt = [\n",
        "    {'role': 'developer', 'content': qna_system_message},\n",
        "    {'role': 'user', 'content': qna_user_message_template.format(\n",
        "         context=context_for_query,\n",
        "         question=user_query\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=prompt,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    prediction = response.choices[0].message.content.strip()\n",
        "except Exception as e:\n",
        "    prediction = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHXY6BcV676h",
        "outputId": "fe74ea1c-3f46-47b4-bde1-3722dd1ca03a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The annual revenue of the company in 2022 was $81.46 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A RAG Assistant"
      ],
      "metadata": {
        "id": "9DuBmLD4wgLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us put together the code in this notebook into a file `rag-chat.py` that will open up a basic command line chat interface whenever it is run at the terminal. This naive implementation neverthless illustrates how document Q&A could be automated."
      ],
      "metadata": {
        "id": "y1B5jZsbxAFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['openai_api_key'] = openai_api_key"
      ],
      "metadata": {
        "id": "0D52ShVX00j7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag-chat.py\n",
        "import os\n",
        "import chromadb\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "model_name = 'gpt-4o-mini'\n",
        "tesla_10k_collection = 'tesla-10k-2019-to-2023'\n",
        "openai_api_key = os.environ.get('openai_api_key')\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        ")\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    api_key=openai_api_key,\n",
        "    model='text-embedding-3-small'\n",
        ")\n",
        "\n",
        "chromadb_client = chromadb.PersistentClient(\n",
        "    path=\"./tesla_db\"\n",
        ")\n",
        "\n",
        "vectorstore_persisted = Chroma(\n",
        "    collection_name=tesla_10k_collection,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    embedding_function=embedding_model,\n",
        "    client=chromadb_client,\n",
        "    persist_directory=\"./tesla_db\"\n",
        ")\n",
        "\n",
        "retriever = vectorstore_persisted.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': 5}\n",
        ")\n",
        "\n",
        "qna_system_message = \"\"\"\n",
        "You are an assistant to a financial services firm who answers user queries on annual reports.\n",
        "User input will have the context required by you to answer user queries.\n",
        "This context will be delimited by: <Context> and </Context>.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User queries will be delimited by: <Question> and </Question>.\n",
        "\n",
        "Please answer user queries only using the context provided in the input.\n",
        "Do not mention anything about the context in your final answer. Your response should only contain the answer to the question.\n",
        "\n",
        "If the answer is not found in the context, respond \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "qna_user_message_template = \"\"\"\n",
        "<Context>\n",
        "Here are some documents that are relevant to the question mentioned below.\n",
        "{context}\n",
        "</Context>\n",
        "\n",
        "<Question>\n",
        "{question}\n",
        "</Question>\n",
        "\"\"\"\n",
        "\n",
        "def respond(user_query):\n",
        "    relevant_document_chunks = retriever.invoke(user_query)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "    context_for_query = \"\\n---\\n\".join(context_list)\n",
        "\n",
        "    prompt = [\n",
        "        {'role': 'developer', 'content': qna_system_message},\n",
        "        {\n",
        "            'role': 'user', 'content': qna_user_message_template.format(\n",
        "             context=context_for_query,\n",
        "             question=user_query)\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=prompt,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        answer = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return answer\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Runs the main interactive loop for the Q&A system.\n",
        "\n",
        "    This function initializes the conversation history, continuously prompts\n",
        "    the user for queries, processes the queries using the `respond` function,\n",
        "    and displays the assistant's responses. It also maintains the\n",
        "    conversation history for context.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Initialize conversation history.\n",
        "    # This list stores the conversation between the user and the assistant.\n",
        "    # It starts with a system message introducing the assistant's role.\n",
        "    conversation_history = [\n",
        "        {'role': 'developer', 'content': 'You are a helpful assistant who answers queries on financial documents'}\n",
        "    ]\n",
        "\n",
        "    # 2. Enter the interactive loop.\n",
        "    # The loop continues until the user enters 'q' to quit.\n",
        "    while True:\n",
        "        # 2.1 Get user input.\n",
        "        # Prompt the user to enter a query and store it in `user_query`.\n",
        "        user_query = input(\"User (type q to quit): \")\n",
        "\n",
        "        # 2.2 Check for quit condition.\n",
        "        # If the user enters 'q', break out of the loop.\n",
        "        if user_query == 'q':\n",
        "            break\n",
        "\n",
        "        # 2.3 Process the query and get the answer.\n",
        "        # Call the `respond` function to process the user query and get the answer.\n",
        "        answer = respond(user_query)\n",
        "\n",
        "        # 2.4 Update conversation history.\n",
        "        # Add the user's query and the assistant's answer to the conversation history.\n",
        "        conversation_history.append({'role': 'user', 'content': user_query})\n",
        "        conversation_history.append({'role': 'assistant', 'content': answer})\n",
        "\n",
        "        # 2.5 Display the assistant's answer.\n",
        "        # Print the assistant's answer to the console.\n",
        "        print(f\"Assistant: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUStmBtIxOPR",
        "outputId": "5dac261a-e893-453c-b540-91154bf2c0eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag-chat.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "largest investment from OEC point of view.\n",
        "what allied product classification have biggest investment.\n",
        "ability to ask questions in data range (this year what has been)."
      ],
      "metadata": {
        "id": "rYwTAaWXYtTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Queries:\n",
        "- What was the total revenue of the company in 2022?\n",
        "- Summarize 5 key risks identified in the 2023 10k report? Respond with bullet point summaries.\n",
        "- What is the view of the management on the future of electric vehicle batteries?\n",
        "- What was the company's debt level in 2023?"
      ],
      "metadata": {
        "id": "Ro6WVZrI3mxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python rag-chat.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWyUupHi0rU6",
        "outputId": "50d47f26-89fe-4dd0-e91c-2cf356b61967"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User (type q to quit): What was the total revenue of the company in 2022?\n",
            "Assistant: In 2022, the total revenues of the company were $81.46 billion.\n",
            "User (type q to quit): Summarize 5 key risks identified in the 2023 10k report? Respond with bullet point summaries.\n",
            "Assistant: - The global COVID-19 pandemic has resulted in macroeconomic conditions that may adversely affect the company, including government regulations and changes in consumer behavior.\n",
            "- Supply chain challenges have been exacerbated by pandemic-related issues, such as port congestion and supplier shutdowns, leading to increased costs and delays in critical parts delivery.\n",
            "- A semiconductor shortfall due to heightened demand for personal electronics has created additional challenges in the company's supply chain and production capabilities.\n",
            "- Labor shortages and absenteeism resulting from the pandemic have impacted operational efficiency and workforce availability.\n",
            "- Cybersecurity threats pose risks to the company's information technology systems, potentially leading to data breaches, reputational damage, and significant liability.\n",
            "User (type q to quit): What is the view of the management on the future of electric vehicle batteries?\n",
            "Assistant: I don't know.\n",
            "User (type q to quit): What was the company's debt level in 2023?\n",
            "Assistant: The company's total debt level in 2023 was $4,683 million.\n",
            "User (type q to quit): q\n"
          ]
        }
      ]
    }
  ]
}