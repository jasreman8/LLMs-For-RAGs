{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasreman8/LLMs-For-RAGs/blob/main/RAG_Based_Q%26A_System_for_PowerBI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOOFZRO7uQRq"
      },
      "source": [
        "# Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfncLsVYbos9"
      },
      "source": [
        "## Implementing a RAG System for Power BI Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGVgYQzanZEq"
      },
      "source": [
        "**Problem Scenario:**\n",
        "\n",
        "In the current data-driven landscape, organizations increasingly rely on powerful analytics tools like Power BI to derive insights and make informed decisions. However, many analysts struggle with the complexity and breadth of Power BI’s official documentation. The extensive resources often lead to confusion, causing users to misinterpret features or overlook essential functionalities. This challenge can result in inefficient data analysis, wasted time, and missed opportunities for actionable insights. Consequently, analysts may not fully leverage the capabilities of Power BI, stifling potential business growth and impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9Gg2LaXuSFT"
      },
      "source": [
        "# Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWbIV8_1ugjj"
      },
      "source": [
        "To address these challenges, we propose implementing a **Retrieval-Augmented Generation (RAG) system** specifically designed for Power BI. This system will enable analysts to formulate questions using natural language and retrieve concise, relevant answers directly from the official documentation. By facilitating better access to critical information, we aim to enhance the operational efficiency of analysts and empower them to utilize Power BI to its fullest potential.\n",
        "\n",
        "The RAG application will simplify interactions with Power BI documentation, allowing users to inquire about specific features, functions, or best practices and receive clear explanations in real-time. By improving understanding and accessibility to the tool, analysts will be able to make quicker, data-driven decisions that lead to a significant business impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0FYFyHAuVEs"
      },
      "source": [
        "# Installing and Importing the Necessary Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvZNmXg_yhYK"
      },
      "source": [
        "In this section, we need to install and import libraries required to run the notebook:\n",
        "\n",
        "- The `openai` package provides the official OpenAI API client for accessing models like GPT-4, Whisper, DALL·E, including its embedding models.\n",
        "\n",
        "- The `tiktoken`\tlibrary provides access to OpenAI's tokenizer models, crucial for chunking and token counting.\n",
        "\n",
        "- The `pypdf` library parses and extracts text from PDF files — useful for document ingestion.\n",
        "\n",
        "- LangChain is a GenAI framework to build applications with LLMs using chains and agents.\n",
        "  - `langchain` is the core library that provides access to various LangChain abstractions\n",
        "  - `langchain-community` provides access to 3rd-party integrations (e.g., different vector stores, tools)\n",
        "  - `langchain-chroma` provides specific integration to use ChromaDB as the vector store backend in LangChain\n",
        "  - `langchain-openai` module provides a plug-in interface for LangChain to call OpenAI's LLMs using standardized interface\n",
        "\n",
        "- `chromadb` library provides access to ChromaDB vector database, which is a fast, vector database optimized for retrieval in RAG systems"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the required libraries\n",
        "!pip install -q openai \\\n",
        "tiktoken \\\n",
        "pypdf \\\n",
        "langchain \\\n",
        "langchain-community \\\n",
        "langchain-chroma \\\n",
        "langchain-openai \\\n",
        "langchain_text_splitters \\\n",
        "chromadb==0.6.3 \\\n",
        "posthog==2.4.2\n"
      ],
      "metadata": {
        "id": "P2VjQSnIe0Oq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCQIO2AiukBJ"
      },
      "source": [
        "**Importing the Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwbwwPJQ9qXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4626c60-e6cd-4f4d-e809-9576e701165e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "# Importing the standard Libraries\n",
        "import time                           # For measuring execution time or adding delays\n",
        "from datetime import datetime         # For handling timestamps and datetime operations\n",
        "\n",
        "# ChromaDB Vector Database\n",
        "import chromadb  # Chroma: a local-first vector database for storing and querying document embeddings\n",
        "\n",
        "# OpenAI SDK\n",
        "from openai import OpenAI\n",
        "# Official OpenAI Python SDK (v1.x) for interacting with models like GPT-4\n",
        "\n",
        "# LangChain Utilities\n",
        "# RecursiveCharacterTextSplitter intelligently breaks long text into smaller chunks with some overlap, preserving context.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Loads all PDF files from a directory and extracts text from each.\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# Base class representing a document in LangChain; useful for downstream chaining and processing.\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Embeddings and Vector Store\n",
        "# Generates vector embeddings using OpenAI’s embedding models (e.g., `text-embedding-3-small`)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Integration for using Chroma as the vector store within LangChain’s ecosystem\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "from google.colab import userdata\n",
        "import os, shutil\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V6F2smbQZD9"
      },
      "source": [
        "## Setup the API Key\n",
        "#### Setup the OpenAI API key and initialize the client with the required model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7GjHml9hGtD"
      },
      "outputs": [],
      "source": [
        "# Set up the OpenAI API Key\n",
        "openai_api_key = userdata.get('my_api_key')\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        ")\n",
        "\n",
        "model_name = 'gpt-4o-mini'\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    api_key=openai_api_key,\n",
        "    model='text-embedding-3-small'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Vq2MLqv82j"
      },
      "source": [
        "## Creating Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ij5gQ6cEljQ",
        "outputId": "5c83778d-a5c1-493f-a304-1116f5926b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  PowerBI.zip\n",
            "  inflating: Introducing_Power_BI.pdf  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the dataset containing the policy document\n",
        "!unzip PowerBI.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RTgOd3CQ1i9"
      },
      "source": [
        "## Load PDF Documents and perform chunking\n",
        "\n",
        "In this step, I will:\n",
        "\n",
        "- Load PDF documents from folder where pdf are saved using PyPDFDirectoryLoader.\n",
        "\n",
        "- Split documents into chunks using RecursiveCharacterTextSplitter with the specified tokenizer, chunk size, and overlap.\n",
        "\n",
        "- Store chunks within LangChain’s Document class.\n",
        "\n",
        "- Inspect contents of the first page by accessing its .page_content attribute.\n",
        "\n",
        "- Define a ChromaDB collection name to store the chunks for later retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7aDdN-MTGWD"
      },
      "outputs": [],
      "source": [
        "# Set the directory where PDF files to be stored\n",
        "pdf_folder_location = \"powerbi_docs\"\n",
        "\n",
        "# Initialize a PDF loader to load all PDF documents in the directory\n",
        "pdf_loader = PyPDFDirectoryLoader(pdf_folder_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfc836dc",
        "outputId": "2ebe2c8f-14b0-4885-868b-38ea3cb49ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved 'Introducing_Power_BI.pdf' to 'powerbi_docs/Introducing_Power_BI.pdf'\n"
          ]
        }
      ],
      "source": [
        "# Define the temporary folder name\n",
        "temp_folder = \"powerbi_docs\"\n",
        "\n",
        "# Create the temporary folder if it doesn't exist\n",
        "if not os.path.exists(temp_folder):\n",
        "    os.makedirs(temp_folder)\n",
        "\n",
        "# Define the source and destination paths for the PDF file\n",
        "source_path = \"Introducing_Power_BI.pdf\"\n",
        "destination_path = os.path.join(temp_folder, \"Introducing_Power_BI.pdf\")\n",
        "\n",
        "# Move the PDF file into the temporary folder\n",
        "shutil.move(source_path, destination_path)\n",
        "\n",
        "print(f\"Moved '{source_path}' to '{destination_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "midS6FrGL_4H",
        "outputId": "adbd605d-cdc1-48e8-a7d1-e1b7881a8b32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.PyPDFDirectoryLoader at 0x7de883719eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pdf_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3S-fmvee_KX"
      },
      "outputs": [],
      "source": [
        "# Splitting documents into chunks using RecursiveCharacterTextSplitter with the specified tokenizer, chunk size, and overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name='cl100k_base',  # OpenAI tokenizer for accurate token count\n",
        "    chunk_size=512,               # Limit each chunk to 512 tokens\n",
        "    chunk_overlap=16              # Keep 16-token overlap between consecutive chunks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M30FYDj0fETt",
        "outputId": "1106e8ef-5aa1-414c-90ba-c7e9ee37d5dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Load the PDF documents and split them into chunks using the text splitter\n",
        "powerbi_chunks = pdf_loader.load_and_split(text_splitter)\n",
        "\n",
        "#Investigating the type of power_bi chunks\n",
        "type(powerbi_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNAFZ5d5fddU",
        "outputId": "6a2a91c4-c83e-4c9e-e9ee-7904c8858a71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "407"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Investigating the length of power_bi chunks\n",
        "len(powerbi_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4Xv-w9fe37",
        "outputId": "1d92b904-85db-4280-dc99-72a33547274c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'Adobe Acrobat Pro 10.1.16', 'creator': 'Adobe Acrobat Pro 10.1.16', 'creationdate': '2016-06-13T10:18:21-04:00', 'author': 'Joan', 'moddate': '2016-06-13T21:13:38-04:00', 'title': '', 'source': 'powerbi_docs/Introducing_Power_BI.pdf', 'total_pages': 407, 'page': 0, 'page_label': '1'}, page_content='Introducing\\nMicrosoft \\nPower BI\\nAlberto Ferrari and Marco Russo')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Investigating the chunks are stored within LangChain's Document class\n",
        "powerbi_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UFT73e5QffGn",
        "outputId": "5b1bf905-0d26-42dd-b4e2-a365458cc680"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Introducing\\nMicrosoft \\nPower BI\\nAlberto Ferrari and Marco Russo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# The individual text chunks can be accessed via the `.page_content` attribute of the `Document` class\n",
        "# Inspecting the contents of the first page by accessing its .page_content attribute\n",
        "powerbi_chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NeU6WEwffJr",
        "outputId": "a0a1106f-2cf7-4e15-f749-2fd89200cb7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'producer': 'Adobe Acrobat Pro 10.1.16',\n",
              " 'creator': 'Adobe Acrobat Pro 10.1.16',\n",
              " 'creationdate': '2016-06-13T10:18:21-04:00',\n",
              " 'author': 'Joan',\n",
              " 'moddate': '2016-06-13T21:13:38-04:00',\n",
              " 'title': '',\n",
              " 'source': 'powerbi_docs/Introducing_Power_BI.pdf',\n",
              " 'total_pages': 407,\n",
              " 'page': 0,\n",
              " 'page_label': '1'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# The metadata corresponding to the first chunk can be accessed via the .metadata attribute\n",
        "powerbi_chunks[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIHUPb0gffME"
      },
      "outputs": [],
      "source": [
        "# Let's define the ChromaDB collection name to store the chunks\n",
        "powerbi_collection = 'powerbi-health-pulse'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYo_sC3IMRS3"
      },
      "source": [
        "### Initialize the OpenAI embedding model with the API key, endpoint, and embedding model name.\n",
        "In this step, I will:\n",
        "\n",
        "- Instantiate the OpenAI embedding model with my API key, endpoint, and embedding model name.\n",
        "\n",
        "- Initialize a persistent Chroma client for managing embeddings.\n",
        "\n",
        "- Ping the database client using the heartbeat method to confirm the connection is alive.\n",
        "\n",
        "- Verify the database is empty before adding new embeddings.\n",
        "\n",
        "- Create a Chroma vector store to store and retrieve document embeddings.\n",
        "\n",
        "- Confirm the collection creation and that the database has been populated.\n",
        "\n",
        "- Batch process 500 chunks at a time when sending to the API, and pause execution for 30 seconds after each batch to avoid rate limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d24vHwcOMWRU"
      },
      "outputs": [],
      "source": [
        "# Instantiating the OpenAI embedding model\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    api_key = openai_api_key, # OpenAI API key\n",
        "    model = 'text-embedding-3-small' # OpenAI's lightweight and cost-effective embedding model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-mbBlDxXDaA"
      },
      "outputs": [],
      "source": [
        "# Initialize a persistent Chroma client\n",
        "chromadb_client = chromadb.PersistentClient(\n",
        "    path=\"./powerbi_db\", # Local directory where vector database will be stored\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV1frfdyXIja",
        "outputId": "ad08f49f-74dd-4c5c-c939-a8e4e757afa7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1768435323215648187"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Pinging the database client to check if the connection is alive\n",
        "# the heartbeat method returns the current time in nanoseconds and is generally used to check if the server is alive\n",
        "chromadb_client.heartbeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29Eyi_BXMXo",
        "outputId": "b41bedbd-5fb4-4424-8410-b0e3ca129f2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['powerbi-health-pulse']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Confirm database is empty\n",
        "chromadb_client.list_collections()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVqo4CLXPepx",
        "outputId": "d64b21b7-32d4-4c7b-c032-0ff92c85b13c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chromadb_client.count_collections()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVUlGTxaXUgd"
      },
      "outputs": [],
      "source": [
        "# Instantiate a Chroma vector store to store and retrieve document embeddings\n",
        "vectorstore = Chroma(\n",
        "    collection_name = powerbi_collection,               # Name of the Chroma collection to group related embeddings\n",
        "    collection_metadata = {\"hnsw:space\": \"cosine\"},     # Use cosine similarity for nearest-neighbor search (HNSW index)\n",
        "    embedding_function = embedding_model,               # Embedding model used to convert text into vectors\n",
        "    client = chromadb_client,                           # Persistent Chroma client initialized earlier\n",
        "    persist_directory = \"./powerbi_db\"                  # Directory where Chroma will persist its data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAFBJroEXXjf",
        "outputId": "dbe24346-8055-48d7-c9b8-f9aef0baf691"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['powerbi-health-pulse']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# confirm collection creation\n",
        "chromadb_client.list_collections()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQj6D2SjPVhf",
        "outputId": "b4bc0300-a54f-43aa-b746-bc4a8c65e98e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Confirm database has been populated with the collection\n",
        "chromadb_client.count_collections()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaSUY_DGX1lZ"
      },
      "outputs": [],
      "source": [
        "# Batch 500 chunks to send to the API at a time, pausing execution for 30 seconds afterward\n",
        "i = 0 # Initialize the starting index for the chunks\n",
        "\n",
        "while i < len(powerbi_chunks): # Iterate while the index is less than the total number of chunks\n",
        "    vectorstore.add_documents( # Add documents to the vector store in batches of 500\n",
        "        documents=powerbi_chunks[i:i+500], # Get the current batch of 500 chunks\n",
        "        ids=[\"text_\" + str(i) for i in range(i, i+500)] # Assign unique IDs to each chunk in the batch\n",
        "    )\n",
        "\n",
        "    i += 500 # Move to the next batch by incremening the index by 500\n",
        "    time.sleep(30) # Pause for 30 seconds to avoid rate limiting issues with the vector store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSyMThOCv3tu"
      },
      "source": [
        "# CRUD Operations in ChromaDB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBq1KyS3HrwX"
      },
      "source": [
        "## **READ**\n",
        "\n",
        "Once the database is created, the stored entries can be retrieved by initializing a new Chroma instance (denoted as **vectorstore_persisted** to distinguish between creation and read operations) and directing it to the persistent storage directory containing the document embeddings.\n",
        "\n",
        "In this step, I will:\n",
        "\n",
        "Initialize a new Chroma instance (e.g., vectorstore_persisted) and point it to the persistent storage directory where embeddings are stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB-G-lAENQrC"
      },
      "outputs": [],
      "source": [
        "# Initializing a new Chroma instance denoted as vectorstore_persisted\n",
        "vectorstore_persisted = Chroma(\n",
        "    collection_name = powerbi_collection,\n",
        "    collection_metadata = {\"hnsw:space\": \"cosine\"},\n",
        "    embedding_function = embedding_model,\n",
        "    client = chromadb_client,\n",
        "    persist_directory = \"./powerbi_db\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJB9N2-YdpBH"
      },
      "source": [
        "There are two valuable types of READ operations in vector databases:\n",
        "\n",
        "1. **Inspecting individual records**\n",
        "2. **Retrieving relevant records based on a user query**\n",
        "\n",
        "In this step, I will:\n",
        "\n",
        " - Define the Chroma collection to work with.\n",
        "\n",
        " - Count the number of records present in the collection.\n",
        "\n",
        "Inspect the first 2 records using the .peek() method to confirm that embeddings have been stored correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr2N7BmFNdLC"
      },
      "source": [
        "**Inspecting individual records**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awinepx4NYK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3755a247-7108-4715-d896-b6c2496962f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "407"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Define the chroma collection\n",
        "collection = chromadb_client.get_collection(powerbi_collection)\n",
        "\n",
        "# Count the number of records in the collection\n",
        "collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5dj8sAWNm4n",
        "outputId": "1ffed7a6-1ae7-4109-d0f6-8d0298d78f20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['text_0', 'text_1'],\n",
              " 'embeddings': array([[-0.0141233 , -0.03104495,  0.0387464 , ..., -0.00077508,\n",
              "          0.00032513,  0.01716083],\n",
              "        [ 0.0097238 ,  0.00484162,  0.02150327, ..., -0.019948  ,\n",
              "         -0.01871731,  0.01614773]]),\n",
              " 'documents': ['Introducing\\nMicrosoft \\nPower BI\\nAlberto Ferrari and Marco Russo',\n",
              "  'PUBLISHED BY \\nMicrosoft Press \\nA division of Microsoft Corporation \\nOne Microsoft Way \\nRedmond, Washington 98052-6399 \\nCopyright © 2016 by Microsoft Corporation \\nAll rights reserved. No part of the contents of \\nthis book may be reproduced or transmitted in \\nany form or by any means without the written \\npermission of the publisher. \\nISBN: 978-1-5093-0228-4 \\nMicrosoft Press books are available through \\nbooksellers and distributors worldwide. If you \\nneed support related to this book, email \\nMicrosoft Press Support at \\nmspinput@microsoft.com. Please tell us what \\nyou think of this book at http://aka.ms/tellpress. \\nThis book is provided “as-is” and expresses the \\nauthor’s views and opinions. The views, opinions \\nand information expressed in this book, \\nincluding URL and other Internet website \\nreferences, may change without notice. \\nSome examples depicted herein are provided for \\nillustration only and are fictitious. No real'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Joan',\n",
              "   'creationdate': '2016-06-13T10:18:21-04:00',\n",
              "   'creator': 'Adobe Acrobat Pro 10.1.16',\n",
              "   'moddate': '2016-06-13T21:13:38-04:00',\n",
              "   'page': 0,\n",
              "   'page_label': '1',\n",
              "   'producer': 'Adobe Acrobat Pro 10.1.16',\n",
              "   'source': 'powerbi_docs/Introducing_Power_BI.pdf',\n",
              "   'title': '',\n",
              "   'total_pages': 407},\n",
              "  {'author': 'Joan',\n",
              "   'creationdate': '2016-06-13T10:18:21-04:00',\n",
              "   'creator': 'Adobe Acrobat Pro 10.1.16',\n",
              "   'moddate': '2016-06-13T21:13:38-04:00',\n",
              "   'page': 1,\n",
              "   'page_label': '2',\n",
              "   'producer': 'Adobe Acrobat Pro 10.1.16',\n",
              "   'source': 'powerbi_docs/Introducing_Power_BI.pdf',\n",
              "   'title': '',\n",
              "   'total_pages': 407}],\n",
              " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
              "  <IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Inspect the first 2 records using the .peek() method\n",
        "collection.peek(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwVQ3Q8R9FKM",
        "outputId": "28104a1d-1d61-4e9a-eee6-b13f36139e7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['text_234'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['215 CH A P T E R  5  |  Getting data from services and  \\n content packs \\n \\n \\nFigure 5-20: The Create Content Pack dialog box \\nreq\\nuires you to select objects to publish in a new \\ncontent pack. \\nWhen you click the Publish button, the content \\npack is published and displayed in the list of the \\ncontent packs that you can obtain by selecting \\nthe View Content Pack item in the Settings menu \\n(refer to Figure 5-19).'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Joan',\n",
              "   'creationdate': '2016-06-13T10:18:21-04:00',\n",
              "   'creator': 'Adobe Acrobat Pro 10.1.16',\n",
              "   'moddate': '2016-06-13T21:13:38-04:00',\n",
              "   'page': 234,\n",
              "   'page_label': '235',\n",
              "   'producer': 'Adobe Acrobat Pro 10.1.16',\n",
              "   'source': 'powerbi_docs/Introducing_Power_BI.pdf',\n",
              "   'title': '',\n",
              "   'total_pages': 407}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Inspect a specific record in the collection\n",
        "collection.get(\n",
        "    ids = ['text_234']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y9032RZq4yR"
      },
      "source": [
        "## Observations\n",
        "The RAG chatbot was made to be scalable and resumable.\n",
        " - The PowerBI documents are ingested and embedded.\n",
        " - The Chroma loads the existing vectors instantly.\n",
        " - During retrieval, the retriever queries vectorstore_persisted for the top-k most semantically similar chunks to a user's question.\n",
        " - While inspecting individual records, it was observed that there were 407 records.\n",
        " - First two records were inspected using peek method, and a specific random record was inspected too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuvUCAL0N2_3"
      },
      "source": [
        "**Retrieving relevant records based on a user query**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws9P7kcod9dE"
      },
      "source": [
        "The primary function of the vector database is to retrieve relevant records based on user queries and to facilitate this process, we implement a retriever that utilizes the query embeddings to query the database.\n",
        "\n",
        "Write code that uses HNSW algorithm to calculate the nearest neighbors for the user query and returns the corresponding documents from the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUXjsv_wN_R8"
      },
      "outputs": [],
      "source": [
        "# Create a retriever interface from the vector store\n",
        "retriever = vectorstore_persisted.as_retriever(\n",
        "    search_type = 'similarity',             # Use the default method based on semantic similarity\n",
        "    search_kwargs = {'k': 5}                # Retrieve top 5 most similar chunks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkj8Q6mPVCkO"
      },
      "outputs": [],
      "source": [
        "# Define a sample user query\n",
        "user_query = \"How can I create a calculated measure in Power BI using DAX?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWgZ01rWc-51"
      },
      "source": [
        "Write code to  performs the similarity search based on the user query by using the `.invoke()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW8ZSXNtc-OR",
        "outputId": "b66b7b15-5bda-4e33-a94b-196344949be0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='text_227', metadata={'author': 'Joan', 'creationdate': '2016-06-13T10:18:21-04:00', 'creator': 'Adobe Acrobat Pro 10.1.16', 'moddate': '2016-06-13T21:13:38-04:00', 'page': 227, 'page_label': '228', 'producer': 'Adobe Acrobat Pro 10.1.16', 'source': 'powerbi_docs/Introducing_Power_BI.pdf', 'title': '', 'total_pages': 407}, page_content='208 CH A P T E R  5  |  Getting data from services and  \\n content packs \\n \\nBecause David used Power Pivot for Excel in the \\npast, he already knows how to write the measure \\nhe needs. So, on the ribbon, on the Home tab, \\nhe clicks New Measure and inserts the following \\nDAX measure in the formula bar: \\nNew Users Growth = \\nIF ( \\n    HASONEVALUE ( Website[Year] ), \\n    DIVIDE ( \\n        SUM ( Website[New Users] ), \\n        CALCULATE ( \\n            SUM ( Website[New Users] ), \\n            Website[Year] = VALUES ( Website[Year] \\n) - 1 \\n        ) \\n    ) \\n) \\nThen, he displays this measure in a separate \\nvisualization, under the New Users metric, \\ngrouped by country/region and year, as \\nillustrated in Figure 5-17.'),\n",
              " Document(id='text_256', metadata={'author': 'Joan', 'creationdate': '2016-06-13T10:18:21-04:00', 'creator': 'Adobe Acrobat Pro 10.1.16', 'moddate': '2016-06-13T21:13:38-04:00', 'page': 256, 'page_label': '257', 'producer': 'Adobe Acrobat Pro 10.1.16', 'source': 'powerbi_docs/Introducing_Power_BI.pdf', 'title': '', 'total_pages': 407}, page_content='237 C H A P T E R  6  |  Building a data model \\n \\nthe price would not take into consideration the \\nquantity sold. \\nNote The Sales table in the Budget – Start.pbix \\nfile is hidden by default because all of its \\ncolumns are hidden. To make it visible, in Data \\nView, right-click the table, and then, on the \\nshortcut menu that opens, click Unhide All. \\nThe default summarization used by Power BI \\nworks perfectly well when you have a simple \\ndata model. But, as soon as you begin loading \\ndata from relational databases for which \\nnumbers are not stored in such a way as to be \\nused in Excel workbooks, you need to stop using \\ndefault summarization and begin writing DAX \\nmeasures, instead. Measures, in DAX parlance, \\nare scripts that you write using DAX-specific \\nsyntax. By using measures, you can author your \\nown code and produce much more powerful \\ndata models. \\nDavid creates a simple measure to compute the \\nSales Amount. In Report View, in the Fields pane, \\nDavid right-clicks the Sales table and then clicks \\nNew Measure. In the formula bar above the \\ncanvas in the middle pane, he replaces “Measure \\n=” with the following code: \\nSales Amount = SUMX ( Sales, Sales[Quantity] * \\nSales[Unit Price] )'),\n",
              " Document(id='text_282', metadata={'author': 'Joan', 'creationdate': '2016-06-13T10:18:21-04:00', 'creator': 'Adobe Acrobat Pro 10.1.16', 'moddate': '2016-06-13T21:13:38-04:00', 'page': 282, 'page_label': '283', 'producer': 'Adobe Acrobat Pro 10.1.16', 'source': 'powerbi_docs/Introducing_Power_BI.pdf', 'title': '', 'total_pages': 407}, page_content='263 C H A P T E R  6  |  Building a data model \\n \\nmodel by yourself. Power BI Desktop offers \\nyou all the tools required to build a complex \\ndata model. \\n\\uf0b7 The DAX language DAX is your best friend \\nin the process of analyzing data. In this \\nchapter, we used it to create calculated \\ncolumns, measures, and calculated tables. \\nThis book is not the proper venue for \\nexplaining how DAX works; that would fill an \\nentire book by itself. If you are interested in \\nlearning more about DAX, check out our \\nbook The Definitive Guide to DAX (Microsoft \\nPress, 2015). \\n\\uf0b7 Building columns for specific charts \\nSometimes, you need a column for an \\nindividual chart. There is nothing wrong with \\ndoing this; you can just build it. \\nBy using some basic skills, you can take Power BI \\nfrom a simple reporting tool to what it really is: \\nan extremely powerful modeling tool with which \\nyou can build gorgeous analyses on top of your \\ndata.'),\n",
              " Document(id='text_322', metadata={'author': 'Joan', 'creationdate': '2016-06-13T10:18:21-04:00', 'creator': 'Adobe Acrobat Pro 10.1.16', 'moddate': '2016-06-13T21:13:38-04:00', 'page': 322, 'page_label': '323', 'producer': 'Adobe Acrobat Pro 10.1.16', 'source': 'powerbi_docs/Introducing_Power_BI.pdf', 'title': '', 'total_pages': 407}, page_content='303 C H A P T E R  7  |  Improving Power BI reports \\n \\nYou have seen how custom visualizations can \\nimprove the report, and sometimes they are \\nnecessary to achieve the desired graphical result. \\nFrom time to time, though, you might still need \\nto massage the data model to present measures \\nand attributes to visualizations in the expected \\nway, using the right granularity, and the \\nexpected formatting. The DAX language is your \\nbest friend here, as you will see in the next \\nsection. \\nUsing DAX in data \\nmodels \\nSo far in this chapter, we have presented several \\nexamples of visualizations. We’ve demonstrated \\nhow you can improve your reports by choosing \\nthe right visualization, setting the properties in \\nan appropriate way, and even installing custom \\nvisualizations when required. However, there are \\na number of improvements that you can achieve \\nthat do not require a direct action on the \\nvisualizations. You can instead create measures \\nor calculated columns in DAX. Usually, you use \\nDAX to obtain a certain numeric result, but'),\n",
              " Document(id='text_323', metadata={'author': 'Joan', 'creationdate': '2016-06-13T10:18:21-04:00', 'creator': 'Adobe Acrobat Pro 10.1.16', 'moddate': '2016-06-13T21:13:38-04:00', 'page': 323, 'page_label': '324', 'producer': 'Adobe Acrobat Pro 10.1.16', 'source': 'powerbi_docs/Introducing_Power_BI.pdf', 'title': '', 'total_pages': 407}, page_content='304 C H A P T E R  7  |  Improving Power BI reports \\n \\nsometimes you can take advantage of a DAX \\nexpression to control the report layout. \\nOur first example concerns the measures used in \\nthe candlestick charts of the report shown in \\nFigure 7-28. Every time period displayed involves \\nfour measures: Open, Close, High, and Low. The \\ndata recorded in the data model has four \\ncorresponding columns for each day and stock. \\nHowever, you might use the candlestick chart to \\ndisplay data by week or by month instead of by \\nday. The aggregation required for each measure \\ndepends on the measure itself. The Open \\nmeasure’s price must be the DayOpen value of \\nthe first day in the period, the Close price must \\nbe the DayClose value of the last day in the \\nperiod, the High price must be the maximum \\nvalue of DayHigh in the period, and the Low \\nprice must be the minimum value of DayLow in \\nthe period. You can write these four measures by \\nusing the following DAX expressions: \\nOpen = \\nIF ( \\n    \\nHASONEVALUE( StocksPrices[Date] ), \\n    VALUES ( StocksPrices[DayOpen] ), \\n    CALCULATE ( VALUES ( StocksPrices[DayOpen] ), \\nFIRSTDATE ( StocksPrices[Date] ) ) \\n)')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Perform similarity search to return the top 5 document chunks based on the sample user query\n",
        "# Replace 'user_query' with the actual query you want to search for\n",
        "retriever.invoke(user_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXhOdqEeraId"
      },
      "source": [
        "## Observations\n",
        "At the retrieval stage of the RAG pipeline, we can start testing how well the PowerBI documentation embeddings work.\n",
        " - The retriever embeds this query.\n",
        " - It searches the Chroma collection for top 5 semantically similar chunks (based on Cosine similarity) related to creating DAX measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgEW08VSOkeT"
      },
      "source": [
        "# RAG Q&A System for PowerBI Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bguX4PvpHekJ"
      },
      "source": [
        "A typical RAG implementation consists of the following stages:\n",
        "* Indexing Stage\n",
        "* Retrieval Stage\n",
        "* Generation Stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd5GR4B3H1zE"
      },
      "source": [
        "| Stage          | Key Activities                                        | Role in RAG                              |\n",
        "| -------------- | ----------------------------------------------------- | ---------------------------------------- |\n",
        "| **Indexing**   | Chunking · Embedding · Storing                        | Prepares data for efficient retrieval    |\n",
        "| **Retrieval**  | Query embedding · Similarity search   | Consolidates relevant context            |\n",
        "| **Generation** | Prompt construction · LLM generation | Produces final response grounded in data |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4qDCb5GH4Jy"
      },
      "source": [
        "Let's now put together the RAG pipeline using these stages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhyBa2NjcbHe"
      },
      "source": [
        "## Retrieval Stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BllDU_R8Er17"
      },
      "source": [
        "**Retrieving Relevant Documents**\n",
        "\n",
        "Write code that performs the Retrieval stage in the RAG pipeline.\n",
        "\n",
        " define a sample user query to test the RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtLFEiXKc3ou"
      },
      "outputs": [],
      "source": [
        "user_query = 'What is Direct Lake mode in Power BI and how is it different from DirectQuery?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbPrX-29EwT6"
      },
      "source": [
        "Retrieve the relevant chunks from the documents based on the `user_query`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okf9mGZNYdMa",
        "outputId": "072c407f-fbd1-4f30-ed94-7c941405f903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "relevant_document_chunks = retriever.invoke(user_query)\n",
        "len(relevant_document_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXbIvKgcc-2D",
        "outputId": "884f45a9-5db1-4c5b-d7ff-23c863f56650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138 C H A P T E R  4  |  Using Power BI Desktop \n",
            " \n",
            "At first glance, it looks like DirectQuery is the \n",
            "most convenient method for loading data, but \n",
            "this is not totally true. If the data is updated \n",
            "frequently, it is very likely that one minute you \n",
            "will see a report with a set of figures, but when \n",
            "you open it again a few minutes later, the \n",
            "numbers might no longer be the same. This is \n",
            "frustrating if you are analyzing information over \n",
            "the span of an entire year (which is what David is \n",
            "doing). Numbers that change too frequently can \n",
            "become disturbing. Also, although real-time data \n",
            "might sometimes be useful, it comes at the cost \n",
            "of query speed; DirectQuery by its very nature is \n",
            "much slower than working with data that is \n",
            "resident on your device and directly accessible \n",
            "by Power BI Desktop. \n",
            "As a final note, keep in mind that DirectQuery \n",
            "works fine when you use Power BI Desktop on \n",
            "your laptop, but when you publish the model to \n",
            "Power BI, the cloud service needs a way to \n",
            "communicate with the internal database server. \n",
            "This is accomplished by using the Enterprise \n",
            "Gateway, which is the advanced version of the \n",
            "Personal Gateway, to which you were introduced \n",
            "in Chapter 3.\n"
          ]
        }
      ],
      "source": [
        "# Inspecting the first document\n",
        "for document in relevant_document_chunks:\n",
        "    print(document.page_content.replace(\"\\t\", \" \"))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5RgCKaScahT"
      },
      "source": [
        "## Generation Stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1EpXcixFL1v"
      },
      "source": [
        "This section will perform the **Generation** stage of the RAG pipeline.\n",
        "\n",
        "We will pass the relevant context chunks to the LLM, along with the system message and user message via a prompt template.\n",
        "\n",
        "These are then passed to the LLM to compose an appropriate response to the user's query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSgPj8vtcbQp"
      },
      "source": [
        "### Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxvNsmoNIbA3"
      },
      "source": [
        "Define the system message for the RAG chatbot with the appopriate role, context and the relevant instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbhiaR6qcu9_"
      },
      "outputs": [],
      "source": [
        "qna_system_message = \"\"\"\n",
        "You are a Power BI expert assisting a business division. Answer user questions about Power BI features and practices.\n",
        "STRICT RULES:\n",
        "1) Use ONLY the information provided between <Context>...</Context>. Do not use outside knowledge or speculate.\n",
        "2) If the answer cannot be found, reply exactly: I don't know\n",
        "3) Do not reference or mention the existence of any context, documents, sources, or citations.\n",
        "4) Be concise and practical. Prefer step-by-step instructions or bullet points when helpful.\n",
        "5) If the question is ambiguous or missing key details, ask ONE clarifying question first, then wait for the user’s reply.\n",
        "\n",
        "FORMATTING:\n",
        "- Plain text or Markdown only.\n",
        "- If you include code, use proper fenced blocks:\n",
        "  - DAX: ```DAX\n",
        "  - Power Query (M): ```PowerQuery\n",
        "- Use numbered steps for procedures; short bullets for options/constraints.\n",
        "- Avoid filler, apologies, or meta commentary.\n",
        "\n",
        "POWER BI SCOPE HINTS (if present in the context):\n",
        "- Calculations: measures vs calculated columns; common DAX (CALCULATE, FILTER, TOTALYTD, ALL, RELATED).\n",
        "- Modeling: relationships (single/bidirectional), star schema, composite models.\n",
        "- Performance: Import vs DirectQuery vs Direct Lake; Performance Analyzer basics.\n",
        "- Security: row-level security (RLS), dynamic roles.\n",
        "- Refresh: gateways, scheduled refresh, incremental refresh.\n",
        "\n",
        "Answer policy summary: answer strictly from <Context>; otherwise say I don't know; keep it short and useful.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnyIL8-RIiyP"
      },
      "source": [
        "Write the user message prompt template that provides the relevant chunks and the user query within the `context` and `question` placeholders respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DGOFaltczH1"
      },
      "outputs": [],
      "source": [
        "qna_user_message_template = \"\"\"\n",
        "<Context>\n",
        "Here are some documents that are relevant to the question mentioned below:\n",
        "{context}\n",
        "</Context>\n",
        "\n",
        "<Question>\n",
        "{question}\n",
        "</Question>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEPtQC14IZJj"
      },
      "source": [
        "### Generating the Response\n",
        "In this step, you need to:   \n",
        "\n",
        "Prompt construction, LLM API call with error handling, and response parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66KLQAE5dH3w",
        "outputId": "1b329d74-0801-489e-f2f8-e20e0248128d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Open Power BI Desktop** and load your dataset.\n",
            "2. **Go to the Modeling tab** in the ribbon.\n",
            "3. **Select Manage Roles** to create security roles.\n",
            "4. **Click on Create** to define a new role.\n",
            "5. **Set DAX filters** for each table to specify which rows users in this role can see. For example, to restrict access to sales data for China, you might use a DAX expression like:\n",
            "   ```DAX\n",
            "   [Country] = \"China\"\n",
            "   ```\n",
            "6. **Add members** to the role by selecting the role and clicking on \"Add members.\"\n",
            "7. **Publish the dataset** to the Power BI service.\n",
            "8. **In the Power BI service**, go to the dataset settings and select the Security option to manage roles and assign users.\n",
            "\n",
            "Note: Ensure that row-level security is applied only to datasets created in Power BI Desktop, as it cannot be applied to datasets created with Power Pivot for Excel.\n"
          ]
        }
      ],
      "source": [
        "user_query = 'How do I implement row-level security for users in a Power BI dataset?'\n",
        "\n",
        "relevant_document_chunks = retriever.invoke(user_query)\n",
        "context_list = [document.page_content for document in relevant_document_chunks]\n",
        "context_for_query = \"\\n---\\n\".join(context_list)\n",
        "\n",
        "prompt = [\n",
        "    {'role':'developer', 'content':qna_system_message},\n",
        "    {'role':'developer', 'content':qna_user_message_template.format(context=context_for_query, question=user_query)}\n",
        "]\n",
        "\n",
        "try:\n",
        "  response = client.chat.completions.create(\n",
        "      model = model_name,\n",
        "      messages = prompt,\n",
        "      temperature = 0\n",
        "  )\n",
        "  prediction = response.choices[0].message.content.strip()\n",
        "except Exception as error:\n",
        "  prediction = f'Sorry, I encountered the following error: \\n {error}'\n",
        "\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "028Vi0kCrDhA"
      },
      "source": [
        "## Observation\n",
        "This is the core \"generation\" step of RAG pipeline where the retrieved document chunks (from the retriever) are sent to the LLM to produce the final natural-language answer to the user's question.\n",
        "\n",
        "- A system prompt (qna_system_message) is defined to constrain the model's behavior.\n",
        "- The retrieved document chunks and user questions are formatted into a structured input.\n",
        "- The OpenAI chat model is called to generate an answer.\n",
        "- The final response is printed.\n",
        "\n",
        "It is the generation part of the RAG flow:\n",
        "\n",
        "Retrieval --> Context formatting --> Generation (LLM call)\n",
        "\n",
        "This block of code feeds the retrieved PowerBI documentation into the LLM, wraps it with clear boundaries and instructions, and returns a clean, grounded answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTZ3kbxBcYX7"
      },
      "source": [
        "# Putting it all together - PowerBI RAG Q&A Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIF7IW1SHlkL"
      },
      "source": [
        "We'll now put together the relevant codes for the RAG pipeline into a file named `rag-chat.py` to create a basic command-line chat interface which can run via  the terminal.\n",
        "\n",
        "This naive RAG implementation illustrates how document Q&A could be automated for any domain.\n",
        "\n",
        "Write code that use the `%%writefile` magic command specific to Google Colab, which allows the content of a cell to be written directly into a file on the virtual machine's disk.\n",
        "\n",
        "This allows for the creation of scripts, configuration files, or data files within the Colab environment. These files are available during the Colab runtime and are deleted when the runtime is stopped or deleted.\n",
        "\n",
        "The `!python` shell command can be used to execute a Python script (.py files) or commands within the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yt6WcJSdhEa",
        "outputId": "5242995c-fae1-49f9-d0df-8f39a21bc830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_chat.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag_chat.py\n",
        "import os, sys, logging, chromadb\n",
        "from typing import List\n",
        "from openai import OpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.CRITICAL)\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "POWERBI_COLLECTION = \"powerbi-health-pulse\"\n",
        "PERSIST_DIR = \"./powerbi_db\"\n",
        "TOP_K = 5\n",
        "MAX_CHARS_PER_CHUNK = 1600  # light guardrail against token bloat\n",
        "\n",
        "# -------- API key resolution --------\n",
        "def resolve_api_key() -> str:\n",
        "    if len(sys.argv) > 1 and sys.argv[1].strip():\n",
        "        return sys.argv[1].strip()\n",
        "    env_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if env_key:\n",
        "        return env_key\n",
        "    print(\"Error: Provide OpenAI API key as argv[1] or set OPENAI_API_KEY env var.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "OPENAI_API_KEY = resolve_api_key()\n",
        "\n",
        "# -------- Clients --------\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    model=\"text-embedding-3-small\",\n",
        ")\n",
        "\n",
        "chromadb_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
        "\n",
        "vectorstore_persisted = Chroma(\n",
        "    collection_name=POWERBI_COLLECTION,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    embedding_function=embedding_model,\n",
        "    client=chromadb_client,\n",
        "    persist_directory=PERSIST_DIR,\n",
        ")\n",
        "\n",
        "retriever = vectorstore_persisted.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": TOP_K},\n",
        ")\n",
        "\n",
        "# -------- Prompting --------\n",
        "qna_system_message = \"\"\"\n",
        "You are a Power BI expert assisting a business division. Answer user questions about Power BI features and practices.\n",
        "STRICT RULES:\n",
        "1) Use ONLY the information provided between <Context>...</Context>. Do not use outside knowledge or speculate.\n",
        "2) If the answer cannot be found, reply exactly: I don't know\n",
        "3) Do not reference or mention the existence of any context, documents, sources, or citations.\n",
        "4) Be concise and practical. Prefer step-by-step instructions or bullet points when helpful.\n",
        "5) If the question is ambiguous or missing key details, ask ONE clarifying question first, then wait for the user’s reply.\n",
        "\n",
        "FORMATTING:\n",
        "- Plain text or Markdown only.\n",
        "- If you include code, use proper fenced blocks:\n",
        "  - DAX: ```DAX\n",
        "  - Power Query (M): ```PowerQuery\n",
        "- Use numbered steps for procedures; short bullets for options/constraints.\n",
        "- Avoid filler, apologies, or meta commentary.\n",
        "\n",
        "POWER BI SCOPE HINTS (if present in the context):\n",
        "- Calculations: measures vs calculated columns; common DAX (CALCULATE, FILTER, TOTALYTD, ALL, RELATED).\n",
        "- Modeling: relationships (single/bidirectional), star schema, composite models.\n",
        "- Performance: Import vs DirectQuery vs Direct Lake; Performance Analyzer basics.\n",
        "- Security: row-level security (RLS), dynamic roles.\n",
        "- Refresh: gateways, scheduled refresh, incremental refresh.\n",
        "\n",
        "Answer policy summary: answer strictly from <Context>; otherwise say I don't know; keep it short and useful.\n",
        "\"\"\".strip()\n",
        "\n",
        "QNA_USER_MESSAGE_TEMPLATE = \"\"\"\n",
        "<Context>\n",
        "Here are some documents that are relevant to the question mentioned below:\n",
        "{context}\n",
        "</Context>\n",
        "\n",
        "<Question>\n",
        "{question}\n",
        "</Question>\n",
        "\"\"\".strip()\n",
        "\n",
        "def _truncate(s: str, max_chars: int) -> str:\n",
        "    return s if len(s) <= max_chars else s[:max_chars] + \"...\"\n",
        "\n",
        "def build_context(chunks: List[str]) -> str:\n",
        "    # Trim each chunk lightly and join with a separator\n",
        "    safe_chunks = [_truncate(c, MAX_CHARS_PER_CHUNK) for c in chunks]\n",
        "    return \"\\n---\\n\".join(safe_chunks)\n",
        "\n",
        "def respond_to_query(user_query: str) -> str:\n",
        "    try:\n",
        "        # Prefer widely supported API\n",
        "        docs = retriever.invoke(user_query) # Changed from .get_relevant_documents\n",
        "    except Exception as e:\n",
        "        return f\"Sorry, I encountered a retrieval error: {e}\"\n",
        "\n",
        "    context_list = [d.page_content for d in docs] if docs else []\n",
        "    context_for_query = build_context(context_list)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": qna_system_message},\n",
        "        {\"role\": \"user\", \"content\": QNA_USER_MESSAGE_TEMPLATE.format(\n",
        "            context=context_for_query, question=user_query)},\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "        )\n",
        "        answer = resp.choices[0].message.content.strip()\n",
        "        if not answer:\n",
        "            return \"I don't know\"\n",
        "        return answer\n",
        "    except Exception as error:\n",
        "        return f\"Sorry, I encountered the following error:\\n{error}\"\n",
        "\n",
        "def get_sources(user_query: str, k: int = TOP_K):\n",
        "    try:\n",
        "        docs = retriever.invoke(user_query) # Changed from .get_relevant_documents\n",
        "    except Exception:\n",
        "        return []\n",
        "    items = []\n",
        "    for d in docs[:k]:\n",
        "        meta = getattr(d, \"metadata\", {}) or {}\n",
        "        items.append({\n",
        "            \"title\": meta.get(\"title\") or meta.get(\"url\") or \"Source\",\n",
        "            \"url\": meta.get(\"url\"),\n",
        "            \"snippet\": d.page_content[:500] + (\"...\" if len(d.page_content) > 500 else \"\")\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Runs the main interactive loop for the Q&A system.\n",
        "    \"\"\"\n",
        "    print(\"PowerBI RAG Q&A. Type 'q' to quit.\\n\")\n",
        "    try:\n",
        "        while True:\n",
        "            user_query = input(\"User: \").strip()\n",
        "            if user_query.lower() in {\"q\", \"quit\", \"exit\"}:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            answer = respond_to_query(user_query)\n",
        "            print(f\"Assistant: {answer}\\n\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nGoodbye!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIxtiuyH5Jv"
      },
      "source": [
        "Run the script using the `!python` shell command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyFkjk_d6CVI"
      },
      "source": [
        "Fomulate 5 queries on the PowerBI Documentation that will then be used to validate the the Q&A RAG Chatbot and provide the output responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdZeA1GNcbho"
      },
      "source": [
        "**SAMPLE QUESTIONS**\n",
        "\n",
        "- How do I share dashboards securely with other users?\n",
        "- How do I handle missing or null data in Power Query?\n",
        "- How do I share dashboards securely with colleagues in Power BI Service?\n",
        "- What is a Power BI Gateway and when do I need one?\n",
        "- When should I use Import mode vs. DirectQuery mode?\n",
        "- What is a Power BI Gateway and when do I need one?\n",
        "- How can I use REST APIs to manage Power BI workspaces?\n",
        "- What are the key differences between Power BI Desktop and Power BI Service?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayp5qZt6dvDP",
        "outputId": "9a6e1d9e-eb59-4c19-9bed-5640e864692d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PowerBI RAG Q&A. Type 'q' to quit.\n",
            "\n",
            "User: How do I share dashboards securely with other users?\n",
            "Assistant: To share dashboards securely with other users, follow these steps:\n",
            "\n",
            "1. **Identify User Type**:\n",
            "   - **Internal Users**: Users within your organization.\n",
            "   - **External Users**: Users outside your organization.\n",
            "\n",
            "2. **Sharing with Internal Users**:\n",
            "   - Invite them by email or send them the dashboard URL.\n",
            "   - Ensure they are authorized to access the dashboard. If not, they can request permission.\n",
            "\n",
            "3. **Sharing with External Users**:\n",
            "   - Invite them by email only.\n",
            "   - The external user must sign in to Power BI using the same email account used in the invitation.\n",
            "   - If they have not used Power BI before, they can create a free account upon signing in.\n",
            "\n",
            "4. **Using Dashboard Links**:\n",
            "   - You can copy the dashboard URL from the Access tab and send it via your own email if you prefer not to use the Power BI service for notifications.\n",
            "\n",
            "5. **Permissions**:\n",
            "   - By default, users will have read-only access. To allow editing, create a group workspace.\n",
            "\n",
            "6. **Consider Security**:\n",
            "   - Be cautious when sharing sensitive data, especially with external users.\n",
            "\n",
            "User: How do I handle missing or null data in Power Query?\n",
            "Assistant: 1. **Remove Columns**: \n",
            "   - Click the column header of the column you want to remove.\n",
            "   - On the shortcut menu, click **Remove Columns**.\n",
            "\n",
            "2. **Filter Rows**:\n",
            "   - Right-click the cell value that is null.\n",
            "   - Choose **Number Filters** and then select **Does Not Equal** to filter out null values.\n",
            "\n",
            "3. **Remove Unused Columns**:\n",
            "   - If a column is no longer useful (e.g., it contains null values), remove it as you did with other columns.\n",
            "\n",
            "4. **Define Data Types**:\n",
            "   - Ensure that the data type of any new or modified columns is set correctly (e.g., change to **Decimal Number** if aggregating values).\n",
            "\n",
            "5. **Group Data** (if needed):\n",
            "   - You can group the dataset by relevant columns (e.g., CountryRegion and Brand) and perform aggregations like counting months.\n",
            "\n",
            "By following these steps, you can effectively handle missing or null data in Power Query.\n",
            "\n",
            "User: How do I share dashboards securely with colleagues in Power BI Service?\n",
            "Assistant: 1. Open the dashboard in your Power BI account.\n",
            "2. Click the **Share** button located in the upper-right corner of the dashboard.\n",
            "3. Enter the email addresses of the colleagues you want to share the dashboard with.\n",
            "4. Note that recipients will not receive an email notification unless you choose to send a message via the Power BI service.\n",
            "5. Alternatively, you can copy the dashboard link from the **Access** tab and send it using your own email account for better recognition.\n",
            "6. Be aware that shared dashboards will be read-only for the recipients unless you create a group workspace for editing permissions.\n",
            "\n",
            "Ensure that all users have the necessary licenses for Power BI Pro and OneDrive for Business if you are using collaborative features.\n",
            "\n",
            "User: What is a Power BI Gateway and when do I need one?\n",
            "Assistant: A Power BI Gateway is a component that facilitates data refreshes for Power BI models published to the Power BI service. There are two types of gateways:\n",
            "\n",
            "1. **Power BI Personal Gateway**:\n",
            "   - Installed on a user's computer.\n",
            "   - Serves only one user and operates when the user's computer is on.\n",
            "\n",
            "2. **Power BI Enterprise Gateway**:\n",
            "   - Installed on a server within a company.\n",
            "   - Serves all users in the company and is available as long as the server is active (usually 24/7).\n",
            "\n",
            "### When You Need a Power BI Gateway:\n",
            "- **For On-Premises Data Sources**: If your Power BI model connects to on-premises data sources using the Import connection type, you need either a Personal or Enterprise Gateway to refresh the data.\n",
            "- **For DirectQuery**: If you create a model using DirectQuery with on-premises data sources, you need the Power BI Enterprise Gateway.\n",
            "- **For Cloud Data Sources**: If using Azure SQL Database or Azure SQL Data Warehouse, you may not need a gateway for data refresh, as Power BI can connect directly to these sources.\n",
            "\n",
            "User: When should I use Import mode vs. DirectQuery mode?\n",
            "Assistant: - **Use Import mode when:**\n",
            "  - You want to work with a static snapshot of the data that does not need to be updated frequently.\n",
            "  - You prefer faster query performance since the data is stored within Power BI's internal data model.\n",
            "  - You are analyzing data over a longer time span (e.g., a year) and do not require real-time updates.\n",
            "\n",
            "- **Use DirectQuery mode when:**\n",
            "  - You need real-time access to the latest data from the source.\n",
            "  - The data is frequently updated, and you want to ensure you are always viewing the most current information.\n",
            "  - You are okay with potentially slower performance due to the need for live queries to the database.\n",
            "\n",
            "Consider your specific use case and data update frequency to choose the appropriate mode.\n",
            "\n",
            "User: What is a Power BI Gateway and when do I need one?\n",
            "Assistant: A Power BI Gateway is a component that facilitates data refreshes for Power BI models published to the Power BI service. There are two types of gateways:\n",
            "\n",
            "1. **Power BI Personal Gateway**:\n",
            "   - Installed on a user's computer.\n",
            "   - Serves only one user and operates when the user's computer is turned on.\n",
            "\n",
            "2. **Power BI Enterprise Gateway**:\n",
            "   - Installed on a server within a company.\n",
            "   - Can serve all users in the company and is available as long as the server is active (usually 24/7).\n",
            "\n",
            "### When You Need a Power BI Gateway:\n",
            "- **For On-Premises Data Sources**: If your Power BI model connects to on-premises data sources using the Import connection type, you need either a Personal or Enterprise Gateway to refresh the data.\n",
            "- **For DirectQuery**: If you create a model using DirectQuery with on-premises data sources, you need the Power BI Enterprise Gateway.\n",
            "- **For Cloud Data Sources**: If using Azure SQL Database or Azure SQL Data Warehouse, you may not need a gateway for data refreshes, as Power BI can connect directly without one.\n",
            "\n",
            "User: How can I use REST APIs to manage Power BI workspaces?\n",
            "Assistant: I don't know\n",
            "\n",
            "User: What are the key differences between Power BI Desktop and Power BI Web?\n",
            "Assistant: - **Platform**: \n",
            "  - Power BI Desktop is a Windows application running locally on your PC.\n",
            "  - Power BI Web is a cloud service accessed through a web browser.\n",
            "\n",
            "- **Complexity**: \n",
            "  - Power BI Desktop is more complex and offers advanced features for building data models.\n",
            "  - Power BI Web is simpler and more user-friendly for basic tasks.\n",
            "\n",
            "- **Modeling Capabilities**: \n",
            "  - Power BI Desktop provides full modeling capabilities and control over the data model.\n",
            "  - Power BI Web has limited modeling features compared to Desktop.\n",
            "\n",
            "- **Data Sources**: \n",
            "  - Power BI Desktop can load data from multiple sources, including databases like SQL Server and Excel.\n",
            "  - Power BI Web primarily focuses on uploading data and building dashboards.\n",
            "\n",
            "- **Usage**: \n",
            "  - Power BI Desktop is typically used for more advanced tasks and data preparation.\n",
            "  - Power BI Web is used for creating and sharing dashboards and reports online.\n",
            "\n",
            "User: q\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Run the script and pass the API key as a command-line argument\n",
        "!python rag_chat.py \"$openai_api_key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kakEtsMrgYgp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}